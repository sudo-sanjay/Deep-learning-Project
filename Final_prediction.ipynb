{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16929703-ca72-47d6-82ba-4b75459c8230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2f7892c-1555-4cae-a76b-3b2311d6fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to play the audio files\n",
    "from IPython.display import Audio\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "398e04d3-84c4-4307-991d-0234f3fbdcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.text import tokenizer_from_json\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_text_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_text_model = model_from_json(loaded_text_model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3651269d-a558-44f6-983b-e752735767fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_text_model.load_weights(\"text_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1c723e1-34a0-4a7e-9e8a-c41781381a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('model_audio.json', 'r')\n",
    "loaded_audio_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_audio_model = model_from_json(loaded_audio_model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6842b55-a179-40bd-a641-e656f2466ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_audio_model.load_weights(\"audio_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37c76be8-0b18-4371-9387-ee03f0deaa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('tokenizer.json','r')\n",
    "l = json_file.read()\n",
    "json_file.close()\n",
    "tokenizer = tokenizer_from_json(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21143ad3-7401-407a-bd2c-f57208092bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = pd.read_csv(r'features.csv')\n",
    "X = Features.iloc[: ,:-1].values\n",
    "Y = Features['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62315dae-3a57-4f6a-bac3-b5b7e8303880",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15b03d17-5b84-4a8a-88c5-a9a66a05e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'1':'anger', '2':'boredom', '3':'empty', '4':'enthusiasm', '5':'fun', '6':'happiness',\n",
    "       '7':'hate','8': 'love','9': 'neutral', '10':'relief', '11':'sadness','12': 'surprise',\n",
    "       '13':'worry' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14ae9b3e-f6ae-48f9-a61d-962e80cfca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(data):\n",
    "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
    "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "\n",
    "def stretch(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(data, rate)\n",
    "\n",
    "def shift(data):\n",
    "    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
    "    return np.roll(data, shift_range)\n",
    "\n",
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0bb99c3-d562-4de0-831b-41ec5af3dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data, sample_rate):\n",
    "    # ZCR\n",
    "    result = np.array([])\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
    "    result=np.hstack((result, zcr)) # stacking horizontally\n",
    "\n",
    "    # Chroma_stft\n",
    "    stft = np.abs(librosa.stft(data))\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, chroma_stft)) # stacking horizontally\n",
    "\n",
    "    # MFCC\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mfcc)) # stacking horizontally\n",
    "\n",
    "    # Root Mean Square Value\n",
    "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
    "    result = np.hstack((result, rms)) # stacking horizontally\n",
    "\n",
    "    # MelSpectogram\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mel)) # stacking horizontally\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_features(path):\n",
    "    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n",
    "    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
    "    \n",
    "    # without augmentation\n",
    "    res1 = extract_features(data,sample_rate)\n",
    "    result = np.array(res1)\n",
    "    \n",
    "    # data with noise\n",
    "    noise_data = noise(data)\n",
    "    res2 = extract_features(noise_data,sample_rate)\n",
    "    result = np.vstack((result, res2)) # stacking vertically\n",
    "    \n",
    "    # data with stretching and pitching\n",
    "    new_data = stretch(data)\n",
    "    data_stretch_pitch = pitch(new_data, sample_rate)\n",
    "    res3 = extract_features(data_stretch_pitch, sample_rate)\n",
    "    result = np.vstack((result, res3)) # stacking vertically\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77958d7e-e18a-47cf-8d24-b17bfe6f32fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_preprocess(data):\n",
    "    feature = get_features(data)\n",
    "    return(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05cfc436-ad03-4a1a-8cd4-fff56e8b3fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sanja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sanja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "oov_tok = \"<oov_tok>\"\n",
    "\n",
    "def count_vectorizer(corpus):\n",
    "    vectorizer = CountVectorizer(analyzer='word')\n",
    "    corpus_words = vectorizer.fit_transform(list(corpus))\n",
    "    return len(vectorizer.vocabulary_)\n",
    "\n",
    "\n",
    "def get_tokenizer_obj(text_list, num_words):\n",
    "    tokenizer = Tokenizer(lower=True, split=\" \", num_words=num_words, oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(text_list)\n",
    "    return tokenizer, len(tokenizer.word_index)\n",
    "\n",
    "\n",
    "def tokenize_texts_to_sequences(tokenizer, text_list):\n",
    "     return tokenizer.texts_to_sequences(text_list)\n",
    "    \n",
    "def padding_sequences(x_arr, max_len):\n",
    "    x_arr = pad_sequences(x_arr, maxlen=max_len, value=0, padding='post')\n",
    "    return x_arr \n",
    "\n",
    "\n",
    "def get_num_words(text):\n",
    "    return count_vectorizer(text)\n",
    "\n",
    "\n",
    "def get_max_statment_len(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad7478e6-6ed8-4bb6-93fb-c88769a20832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    f_text = tokenize_texts_to_sequences(tokenizer, text)\n",
    "    f_text = padding_sequences(f_text, 46)\n",
    "    return(f_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efbe5419-d2ff-46c9-b2d2-39a2c4866c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanja\\anaconda3\\envs\\ml_everything\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "r = sr.Recognizer()\n",
    "def audio_to_text(path):\n",
    "    with sr.AudioFile(path) as source:\n",
    "        r.adjust_for_ambient_noise(source)\n",
    "        audio = r.listen(source)\n",
    "    try:\n",
    "        print(r.recognize_google(audio))\n",
    "        return(r.recognize_google(audio)) \n",
    "    except Exception as e:\n",
    "        print(\"Error {} : \".format(e) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b778a862-545f-4c30-ae49-fd63f1bd726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_preprocess(data):\n",
    "    text = audio_to_text(data)\n",
    "    final_text = text_preprocess(text)\n",
    "    new_data = audio_preprocess(data)\n",
    "    return([final_text, new_data])\n",
    "    \n",
    "def most_frequent(List):\n",
    "    return max(set(List), key = List.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95088c08-e08e-4e04-a7e4-8d4e4f30297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_predict(data):\n",
    "    text_pred, audio_pred = complete_preprocess(data)\n",
    "    audio_pred_noise, audio_pred_strech, audio_pred_basic = audio_pred[1], audio_pred[2], audio_pred[0] \n",
    "    audio_pred = np.expand_dims(audio_pred, axis=2)\n",
    "    audio_output = loaded_audio_model.predict(audio_pred)\n",
    "    text_output = loaded_text_model.predict(text_pred)\n",
    "    y_pred = np.argmax(text_output, axis=1)\n",
    "    f = most_frequent(list(y_pred))\n",
    "    audio_out = encoder.inverse_transform(audio_output)\n",
    "    print(\"The speaker is {0} and {1}\".format(audio_out[2][0],d[str(f)]))\n",
    "    print(\"The audio model with noisy input {0} and {1}\".format(audio_out[0][0],d[str(f)]))\n",
    "    print(\"The audio model with streched input {0} and {1}\".format(audio_out[1][0],d[str(f)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6457c4d-cb12-4c72-aaad-bcd1775d78f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "describe Priyam beautiful to make this life of wonderful Adventure of democracy that issues that unite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanja\\AppData\\Local\\Temp\\ipykernel_16912\\4130844094.py:7: FutureWarning: Pass rate=0.8 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.effects.time_stretch(data, rate)\n",
      "C:\\Users\\sanja\\AppData\\Local\\Temp\\ipykernel_16912\\4130844094.py:14: FutureWarning: Pass sr=22050, n_steps=0.7 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The speaker is calm and love\n",
      "The audio model with noisy input calm and love\n",
      "The audio model with streched input neutral and love\n"
     ]
    }
   ],
   "source": [
    "data = 'test.wav'\n",
    "final_predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38131db-cfcf-478f-a9d6-8019304619f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
